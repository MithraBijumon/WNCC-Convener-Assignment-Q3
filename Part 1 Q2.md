# Model - Neural Networks
My model uses neural networks to train itself. So what are neural netwroks. Basically imagine you get an input and you don't have any formula but you must get an ouput. You are given many many such inputs. How do you derive a formula? Basically imagining this process through Neural Networks makes it really easy to understand. You take random formulas and perform calculations and arrive at a final answer. Then you compare this with your target output and calculate how lossy your calculation is i.e., how far away your data is from the actual target output. Then using gradients you go backward through your network and then slightly change your formula in the direction away from the loss and again calculate the output. Keep repeating this until your loss is minimized.
1. nn.CrossEntropyLoss(ignore_index=0) is the goated method for really easily calculating this loss. torch internally maintains internal gradients for every method you use unless it's not training(i.e., it's evaluating) or unless specifically mentioned not to include it (like in the case of positional encoding). Now were do you need to make changes in this model? Were can you make changes?
2. Simple... in the linear transfmations parts... were the weights and biases are not provided beforehand. Of course you don't manually need to do it but this is what happens internally.
3. What does criterion = nn.CrossEntropyLoss(ignore_index=0) do? First of all that ignore_index = 0 tells the model to ignore all paddings... note that I had defined paddings at index = 0 in my source vocabulary so that paddings are not included during training. After that the forward function takes in 2 arguments calculated output and target output... the shape of calculated output being [no_of_sentences, no_of_classes] were each sentence shows 6(because here there are 6 classes) scores... each representing the weight of the class it points to. These scores are basically called "logits". So CrossEntropyLoss calculates the softmax for this which now gives the probability for each class. This is then compared with the actual output and a negative log likelihood is calculated which basically represents the loss. These are all internal workings and I did only basic level research for that so I am not able to explain this in detail.
4. Now that I got the loss I used the optimizer "optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)". Here transformer.parameters() tells it what to update, lr(learning rate) is set to 0.0001 so that the model doesn't make drastic changes every iteration. Betas represent the exponential decay rates. (This is not something I could understand again because I didn't do a very deep research on loss calculation). eps=1e-9 basically ensures that division by zero error doesn't occur.
5. optimizer.step() actually updates the model weights using the gradients that were just computed from loss.backward() function. I asked chatGPT to explain this to me and it came up with a really nice way to put this together... "Hey Adam, take the gradients you just saw, and nudge the model weights in the direction that makes the loss go down â€” using a learning rate of 0.0001 and your internal tricks for smoothing things out." ðŸ¤£
6. Now I'll explain what happens at every iteration. First grads for all the parameters are set to zero. Then the model forward propagates and calculates loss using CrossEntropyLoss. From there it calculates grads for all modifiable parameters using loss.backward(). Then it optimizes every parameter in a way that loss is minimized slightly.
7. Running it 100 times ensures loss is minimized.
8. One thing I noticed is that loss saturates after a while which I'll discuss in a while.



# Why this model?
