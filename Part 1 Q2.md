# Model - Neural Networks
My model uses neural networks to train itself. So what are neural netwroks. Basically imagine you get an input and you don't have any formula but you must get an ouput. You are given many many such inputs. How do you derive a formula? Basically imagining this process through Neural Networks makes it really easy to understand. You take random formulas and perform calculations and arrive at a final answer. Then you compare this with your target output and calculate how lossy your calculation is i.e., how far away your data is from the actual target output. Then using gradients you go backward through your network and then slightly change your formula in the direction away from the loss and again calculate the output. Keep repeating this until your loss is minimized.
1. nn.CrossEntropyLoss(ignore_index=0) is the goated method for really easily calculating this loss. torch internally maintains internal gradients for every method you use unless it's not training(i.e., it's evaluating) or unless specifically mentioned not to include it (like in the case of positional encoding). Now were do you need to make changes in this model? Were can you make changes?
2. Simple... in the linear transfmations parts... were the weights and biases are not provided beforehand. Of course you don't manually need to do it but this is what happens internally.
3. What does criterion = nn.CrossEntropyLoss(ignore_index=0) do? First of all that ignore_index = 0 tells the model to ignore all paddings... note that I had defined paddings at index = 0 in my source vocabulary so that paddings are not included during training. After that the forward function takes in 2 arguments calculated output and target output... the shape of calculated output being [no_of_sentences, no_of_classes] were each sentence shows 6(because here there are 6 classes) scores... each representing the weight of the class it points to. These scores are basically called "logits". So CrossEntropyLoss calculates the softmax for this which now gives the probability for each class. This is then compared with the actual output and a negative log likelihood is calculated which basically represents the loss. These are all internal workings and I did only basic level research for that so I am not able to explain this in detail.
4. Now that I got the loss I used the optimizer "optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)". Here transformer.parameters() tells it what to update, lr(learning rate) is set to 0.0001 so that the model doesn't make drastic changes every iteration. Betas represent the exponential decay rates. (This is not something I could understand again because I didn't do a very deep research on loss calculation). eps=1e-9 basically ensures that division by zero error doesn't occur.
5. optimizer.step() actually updates the model weights using the gradients that were just computed from loss.backward() function. I asked chatGPT to explain this to me and it came up with a really nice way to put this together... "Hey Adam, take the gradients you just saw, and nudge the model weights in the direction that makes the loss go down ‚Äî using a learning rate of 0.0001 and your internal tricks for smoothing things out." ü§£
6. Now I'll explain what happens at every iteration. First grads for all the parameters are set to zero. Then the model forward propagates and calculates loss using CrossEntropyLoss. From there it calculates grads for all modifiable parameters using loss.backward(). Then it optimizes every parameter in a way that loss is minimized slightly.
7. Running it 100 times ensures loss is minimized.
8. One thing I noticed is that loss saturates after a while which I'll discuss in a while.



# Why this model?
So I researched briefly about all models mentioned and found Neural Networks easy to visualize and implement. Here's what I found for each
1. Logistic Regression : "Logistic regression is a supervised machine learning algorithm used for classification tasks where the goal is to predict the probability that an instance belongs to a given class or not. Logistic regression is a statistical algorithm which analyze the relationship between two data factors. The article explores the fundamentals of logistic regression, it‚Äôs types and implementations. Logistic regression is used for binary classification where we use sigmoid function, that takes input as independent variables and produces a probability value between 0 and 1. For example, we have two classes Class 0 and Class 1 if the value of the logistic function for an input is greater than 0.5 (threshold value) then it belongs to Class 1 otherwise it belongs to Class 0. It‚Äôs referred to as regression because it is the extension of linear regression but is mainly used for classification problems." Of course I also searched if it works for multi class classification and yes for that instead of the sigmoid function it used the softmax function. But the problem with it is that it can't understand the meaning of the sentence and simple views all words independently. For exmaple it cannot understand that "not bad" and "bad" are two different things.
2. Naive Bayes : "The main idea behind the Naive Bayes classifier is to use Bayes‚Äô Theorem to classify data based on the probabilities of different classes given the features of the data. It is used mostly in high-dimensional text classification. It is named as ‚ÄúNaive‚Äù because it assumes the presence of one feature does not affect other features." Ya so this one has the same problem in the fact that it can't understand the meaning of the sentence
3. Random Forest : "A Random Forest is a collection of decision trees that work together to make predictions." So the main problem with this is that it is limited to a specific number of trees. But neural networks start from root up and goes through all possibilities. Also more the number of trees more will be the computation.
4. SVM : "SVM aims to find the optimal hyperplane in an N-dimensional space to separate data points into different classes. The algorithm maximizes the margin between the closest points of different classes." This is something I didn't really understand. Like visualizing this is of course a problem unlike neural networks.

So with all these factors in mind, I chose the Neural Networks model because I could really understand it, visulize and implement it. Also it can understand the meaning of words and its context which is really helpful in protecting against wrong predictions. 



# Challenges
Now one obvious thing is the saturation of loss. After a while the loss simple doesn't decrease much. Now why could this be? This is beacuse after a point my model simply can't learn more than it already has and my data is no longer useful. So even given huge databases of data can't improve my model performance. I faced this issue personally because my model, how much ever data I gave it always returned CP for "What are some beginner-friendly open-source projects to contribute to?" and the funny thing is open-source is literally written in the sentence. So while making this write up itself I realized why not add a feature in clean to remove that hyphen and split it further so I went ahead and did that but sadly didn't get the result I wanted.
Other challenges include the fact that a lot of computation happens here and the bigger the network, more will be the computations. My network uses 6 layers only. If I were to add more layers or maybe even more heads training it would be tiresome.
